{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bb627d-9ad8-428e-84f8-2c28ba56c3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExplainability analysis for trained Transformer model.\\n\\nTechniques:\\n1. Attention Visualization - Shows which time steps the model focuses on\\n2. Integrated Gradients - Shows which features drive predictions\\n\\nUsage:\\n    python explainability_analysis.py\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Explainability analysis for trained Transformer model.\n",
    "\n",
    "Techniques:\n",
    "1. Attention Visualization - Shows which time steps the model focuses on\n",
    "2. Integrated Gradients - Shows which features drive predictions\n",
    "\n",
    "Usage:\n",
    "    python explainability_analysis.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b188b0d3-3a78-401b-a6c1-f56000dd234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b3d27a-2ec6-4587-96e7-98cc753a0327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your model architecture and dataset\n",
    "# Adjust these imports based on your actual file structure\n",
    "# from your_training_script import TinyMelTransformer, MelTransformerDataset, collate_variable_length\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Config - Update these paths!\n",
    "# ==========================\n",
    "MODEL_PATH = \"/scratch/jbfrantz/jess_explainability_model/jess_outputs/best_female_transformer.pt\"\n",
    "METADATA_PATH = \"/scratch/jbfrantz/jess_explainability_model/jess_outputs/dataset_metadata.npz\"\n",
    "OUT_DIR = \"/scratch/jbfrantz/jess_explainability_model/explainability_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Data paths (same as training)\n",
    "MEL_DIR = \"/scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum\"\n",
    "LABEL_DIR = \"/scratch/sshuvo13/project_shared_folder_bspml_1/rml_analysis/fixed_rml_analysis/labels_again/fixed_30s_label_outputs\"\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_SAMPLES_TO_EXPLAIN = 20  # Number of test samples to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e17749a4-2a89-4f42-a5b3-3762ab99dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_pairs(mel_dir, label_dir):\n",
    "    mel_files = sorted(glob.glob(os.path.join(mel_dir, \"*.npy\")))\n",
    "    print(f\"[INFO] Total mel files found: {len(mel_files)}\")\n",
    "\n",
    "    pairs = []\n",
    "    for mel_path in mel_files:\n",
    "        mel_name = os.path.basename(mel_path)\n",
    "        stem = os.path.splitext(mel_name)[0]  # e.g. 00001006-100507\n",
    "        label_name = f\"{stem}_segments_labels.npy\"\n",
    "        label_path = os.path.join(label_dir, label_name)\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            pairs.append((mel_path, label_path))\n",
    "        else:\n",
    "            print(f\"[WARN] Missing label for {mel_name} -> {label_name}\")\n",
    "\n",
    "    print(f\"[INFO] Usable (mel, label) pairs: {len(pairs)}\")\n",
    "    return pairs\n",
    "\n",
    "def split_file_pairs(file_pairs, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(file_pairs)\n",
    "    n = len(file_pairs)\n",
    "    n_train = int(0.7 * n)\n",
    "    n_val = int(0.2 * n)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_pairs = file_pairs[:n_train]\n",
    "    val_pairs = file_pairs[n_train:n_train + n_val]\n",
    "    test_pairs = file_pairs[n_train + n_val:]\n",
    "\n",
    "    print(f\"[INFO] Patient-level split (files):\")\n",
    "    print(f\"       Train: {len(train_pairs)}\")\n",
    "    print(f\"       Val:   {len(val_pairs)}\")\n",
    "    print(f\"       Test:  {len(test_pairs)}\")\n",
    "    return train_pairs, val_pairs, test_pairs\n",
    "\n",
    "# ==========================\n",
    "# Dataset: Transformer-ready sequences\n",
    "# ==========================\n",
    "class MelTransformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Optimized version: preprocess all segments in __init__ for fast training.\n",
    "    \n",
    "    For each segment:\n",
    "      - mel: (C, n_mels, frames)\n",
    "      - time pooling by factor TIME_POOL\n",
    "      - flatten C and n_mels -> feature_dim = C * n_mels\n",
    "      - output sequence: (T', feature_dim)\n",
    "\n",
    "    Labels:\n",
    "      - Normal -> 0, all other classes -> 1 (event)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_pairs, time_pool=10):\n",
    "        self.time_pool = time_pool\n",
    "        self.sequences = []  # Store preprocessed sequences\n",
    "        self.labels = []     # Store corresponding labels\n",
    "        self.skipped = []\n",
    "        self.label_to_int = None\n",
    "        all_label_strings = []\n",
    "        self.uses_string_labels = False\n",
    "\n",
    "        total_segments = 0\n",
    "        \n",
    "        for mel_path, label_path in file_pairs:\n",
    "            # Load mel - no mmap, load fully into memory\n",
    "            try:\n",
    "                X = np.load(mel_path, allow_pickle=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Skipping MEL file: {mel_path}, reason: {repr(e)}\")\n",
    "                self.skipped.append((mel_path, label_path))\n",
    "                continue\n",
    "\n",
    "            # Load labels\n",
    "            try:\n",
    "                y = np.load(label_path, allow_pickle=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Skipping LABEL file: {label_path}, reason: {repr(e)}\")\n",
    "                self.skipped.append((mel_path, label_path))\n",
    "                continue\n",
    "\n",
    "            if y.ndim > 1:\n",
    "                y = np.squeeze(y)\n",
    "\n",
    "            N_mel = X.shape[0]\n",
    "            N_lab = y.shape[0]\n",
    "            if N_mel != N_lab:\n",
    "                N = min(N_mel, N_lab)\n",
    "                print(\n",
    "                    f\"[WARN] Length mismatch for {mel_path} vs {label_path}: \"\n",
    "                    f\"mel={N_mel}, label={N_lab}. Using first {N} segments.\"\n",
    "                )\n",
    "                X = X[:N]\n",
    "                y = y[:N]\n",
    "            else:\n",
    "                N = N_mel\n",
    "\n",
    "            if y.dtype.kind in {\"U\", \"S\", \"O\"}:\n",
    "                self.uses_string_labels = True\n",
    "                all_label_strings.extend(y.tolist())\n",
    "\n",
    "            # Preprocess ALL segments from this file\n",
    "            for i in range(N):\n",
    "                seq = self._segment_to_sequence(X[i])\n",
    "                self.sequences.append(seq)\n",
    "                self.labels.append(y[i])\n",
    "            \n",
    "            total_segments += N\n",
    "            print(f\"[INFO] Processed {mel_path}: {N} segments, shape={X.shape[1:]}\")\n",
    "\n",
    "        if total_segments == 0:\n",
    "            raise RuntimeError(\"No valid female segments loaded!\")\n",
    "\n",
    "        print(f\"[INFO] Total female segments: {total_segments}\")\n",
    "\n",
    "        # Set up label mapping\n",
    "        if self.uses_string_labels:\n",
    "            unique = sorted(set(all_label_strings))\n",
    "            print(f\"[INFO] Found label classes: {unique}\")\n",
    "            # Normal -> 0, all others -> 1\n",
    "            self.label_to_int = {lab: (0 if lab == \"Normal\" else 1) for lab in unique}\n",
    "            print(\"[INFO] Binary label mapping:\")\n",
    "            print(\"       Normal -> 0\")\n",
    "            for lab in unique:\n",
    "                if lab != \"Normal\":\n",
    "                    print(f\"       {lab} -> 1 (event)\")\n",
    "        else:\n",
    "            self.label_to_int = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def _segment_to_sequence(self, seg):\n",
    "        \"\"\"\n",
    "        seg: (C, n_mels, frames)\n",
    "        Apply time pooling and flatten freq+channel.\n",
    "        Return: (T', feature_dim) float32\n",
    "        \"\"\"\n",
    "        C, n_mels, T = seg.shape\n",
    "\n",
    "        # Time pooling\n",
    "        if self.time_pool > 1 and T >= self.time_pool:\n",
    "            T_new = T // self.time_pool\n",
    "            T_use = T_new * self.time_pool\n",
    "            seg = seg[:, :, :T_use]\n",
    "            seg = seg.reshape(C, n_mels, T_new, self.time_pool).mean(axis=-1)\n",
    "            # (C, n_mels, T_new)\n",
    "        else:\n",
    "            T_new = T\n",
    "\n",
    "        seg = seg.reshape(C * n_mels, T_new)   # (feat_dim, T_new)\n",
    "        seg = seg.transpose(1, 0)             # (T_new, feat_dim)\n",
    "        return seg.astype(np.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Simple lookup - no bisect, no on-the-fly processing!\n",
    "        seq = self.sequences[idx]\n",
    "        y_np = self.labels[idx]\n",
    "        \n",
    "        x = torch.from_numpy(seq)\n",
    "\n",
    "        if self.label_to_int is not None:\n",
    "            y_bin = float(self.label_to_int[str(y_np)])\n",
    "        else:\n",
    "            y_bin = float(y_np)\n",
    "\n",
    "        y = torch.tensor(y_bin, dtype=torch.float32)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "def collate_variable_length(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (seq, label)\n",
    "    seq: (T_i, feat_dim)\n",
    "    Returns:\n",
    "      X_padded: (B, max_T, feat_dim)\n",
    "      y: (B, 1)\n",
    "    \"\"\"\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = [s.shape[0] for s in seqs]\n",
    "    feat_dim = seqs[0].shape[1]\n",
    "\n",
    "    max_T = max(lengths)\n",
    "    B = len(seqs)\n",
    "\n",
    "    X_padded = torch.zeros(B, max_T, feat_dim, dtype=torch.float32)\n",
    "    for i, s in enumerate(seqs):\n",
    "        T_i = s.shape[0]\n",
    "        X_padded[i, :T_i, :] = s\n",
    "\n",
    "    y = torch.stack(labels).view(-1, 1)\n",
    "    return X_padded, y\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Modified Model for Attention Extraction\n",
    "# ==========================\n",
    "class TinyMelTransformerWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Same as TinyMelTransformer but returns attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout, max_len=2000)\n",
    "\n",
    "        # Store encoder layers separately so we can extract attention\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        \n",
    "        self.attention_weights = []  # Store attention from each layer\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        # x: (B, T, feat_dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        self.attention_weights = []\n",
    "        \n",
    "        # Pass through encoder layers and optionally collect attention\n",
    "        for layer in self.encoder_layers:\n",
    "            if return_attention:\n",
    "                # Hook to extract attention weights\n",
    "                def get_attention_hook(module, input, output):\n",
    "                    # TransformerEncoderLayer doesn't directly expose attention,\n",
    "                    # so we need to modify the self_attn module\n",
    "                    pass\n",
    "                \n",
    "                # Note: PyTorch's TransformerEncoderLayer doesn't expose attention by default\n",
    "                # We'll use a workaround by accessing the self_attn module\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        h = x\n",
    "        pooled = h.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits, self.attention_weights if return_attention else None\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Tiny Transformer\n",
    "# ==========================\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, dropout=0.1, max_len=2000):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "#         div_term = torch.exp(\n",
    "#             torch.arange(0, d_model, 2, dtype=torch.float32) * (-np.log(10000.0) / d_model)\n",
    "#         )\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         if d_model % 2 == 1:\n",
    "#             pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "#         else:\n",
    "#             pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer(\"pe\", pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         x: (batch_size, seq_len, d_model)\n",
    "#         \"\"\"\n",
    "#         seq_len = x.size(1)\n",
    "#         x = x + self.pe[:, :seq_len, :]\n",
    "#         return self.dropout(x)\n",
    "\n",
    "\n",
    "# class TinyMelTransformer(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Tiny Transformer over mel-spectrogram time axis.\n",
    "#     Input to forward: (B, T, feat_dim)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_dim,\n",
    "#         d_model=64,\n",
    "#         nhead=4,\n",
    "#         num_layers=2,\n",
    "#         dim_feedforward=128,\n",
    "#         dropout=0.1,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.proj = nn.Linear(input_dim, d_model)\n",
    "#         self.pos_encoder = PositionalEncoding(d_model, dropout=dropout, max_len=2000)\n",
    "\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(\n",
    "#             d_model=d_model,\n",
    "#             nhead=nhead,\n",
    "#             dim_feedforward=dim_feedforward,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True,\n",
    "#         )\n",
    "#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(d_model, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.3),\n",
    "#             nn.Linear(64, 1),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (B, T, feat_dim)\n",
    "#         x = self.proj(x)\n",
    "#         x = self.pos_encoder(x)\n",
    "#         h = self.transformer(x)\n",
    "#         pooled = h.mean(dim=1)\n",
    "#         logits = self.classifier(pooled)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=2000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32) * (-np.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Integrated Gradients\n",
    "# ==========================\n",
    "# def integrated_gradients(model, input_tensor, target_class, baseline=None, steps=50):\n",
    "#     \"\"\"\n",
    "#     Compute Integrated Gradients for a given input.\n",
    "    \n",
    "#     Args:\n",
    "#         model: PyTorch model\n",
    "#         input_tensor: (1, T, feat_dim) - single sample\n",
    "#         target_class: class index to explain (0 or 1)\n",
    "#         baseline: baseline input (default: zeros)\n",
    "#         steps: number of interpolation steps\n",
    "    \n",
    "#     Returns:\n",
    "#         attributions: (T, feat_dim) - importance scores\n",
    "#     \"\"\"\n",
    "#     if baseline is None:\n",
    "#         baseline = torch.zeros_like(input_tensor)\n",
    "    \n",
    "#     # Generate interpolated inputs between baseline and actual input\n",
    "#     alphas = torch.linspace(0, 1, steps).to(input_tensor.device)\n",
    "    \n",
    "#     # Interpolate: baseline + alpha * (input - baseline)\n",
    "#     interpolated_inputs = baseline + alphas.view(-1, 1, 1) * (input_tensor - baseline)\n",
    "    \n",
    "#     # Compute gradients for each interpolated input\n",
    "#     interpolated_inputs.requires_grad_(True)\n",
    "    \n",
    "#     model.eval()\n",
    "#     gradients = []\n",
    "    \n",
    "#     for i in range(steps):\n",
    "#         inp = interpolated_inputs[i:i+1]\n",
    "#         logits, _ = model(inp)\n",
    "        \n",
    "#         # Get gradient for target class\n",
    "#         model.zero_grad()\n",
    "#         logits[0, target_class].backward(retain_graph=True)\n",
    "        \n",
    "#         gradients.append(inp.grad.detach().clone())\n",
    "#         inp.grad.zero_()\n",
    "    \n",
    "#     # Average gradients\n",
    "#     avg_gradients = torch.stack(gradients).mean(dim=0)\n",
    "    \n",
    "#     # Integrated gradients = (input - baseline) * avg_gradients\n",
    "#     attributions = (input_tensor - baseline) * avg_gradients\n",
    "    \n",
    "#     return attributions.squeeze(0)  # (T, feat_dim)\n",
    "\n",
    "def integrated_gradients(model, input_tensor, target_class, baseline=None, steps=50):\n",
    "    \"\"\"\n",
    "    Compute Integrated Gradients for a given input.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        input_tensor: (1, T, feat_dim) - single sample\n",
    "        target_class: class index to explain (0 or 1)\n",
    "        baseline: baseline input (default: zeros)\n",
    "        steps: number of interpolation steps\n",
    "    \n",
    "    Returns:\n",
    "        attributions: (T, feat_dim) - importance scores\n",
    "    \"\"\"\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    # Generate interpolated inputs between baseline and actual input\n",
    "    alphas = torch.linspace(0, 1, steps).to(input_tensor.device)\n",
    "    \n",
    "    model.eval()\n",
    "    gradients = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        # Create interpolated input for this step\n",
    "        alpha = alphas[i]\n",
    "        inp = baseline + alpha * (input_tensor - baseline)\n",
    "        inp.requires_grad_(True)  # Set requires_grad on each individual input\n",
    "        \n",
    "        logits, _ = model(inp)\n",
    "        \n",
    "        # Get gradient for target class\n",
    "        model.zero_grad()\n",
    "        logits[0, target_class].backward()\n",
    "        \n",
    "        gradients.append(inp.grad.detach().clone())\n",
    "    \n",
    "    # Average gradients\n",
    "    avg_gradients = torch.stack(gradients).mean(dim=0)\n",
    "    \n",
    "    # Integrated gradients = (input - baseline) * avg_gradients\n",
    "    attributions = (input_tensor - baseline) * avg_gradients\n",
    "    \n",
    "    return attributions.squeeze(0)  # (T, feat_dim)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Attention Rollout\n",
    "# ==========================\n",
    "def attention_rollout(attention_weights, head_fusion='mean'):\n",
    "    \"\"\"\n",
    "    Combine attention weights across layers.\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: list of attention tensors from each layer\n",
    "                          Each tensor: (batch, num_heads, seq_len, seq_len)\n",
    "        head_fusion: 'mean' or 'max' - how to combine multiple heads\n",
    "    \n",
    "    Returns:\n",
    "        rollout: (seq_len, seq_len) - aggregated attention\n",
    "    \"\"\"\n",
    "    if not attention_weights:\n",
    "        return None\n",
    "    \n",
    "    # Fuse heads\n",
    "    if head_fusion == 'mean':\n",
    "        fused_attention = [attn.mean(dim=1) for attn in attention_weights]\n",
    "    else:\n",
    "        fused_attention = [attn.max(dim=1)[0] for attn in attention_weights]\n",
    "    \n",
    "    # Roll out across layers\n",
    "    rollout = fused_attention[0]\n",
    "    for attn in fused_attention[1:]:\n",
    "        rollout = torch.matmul(attn, rollout)\n",
    "    \n",
    "    return rollout\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Visualization Functions\n",
    "# ==========================\n",
    "def plot_attention_heatmap(attention_matrix, sample_idx, true_label, pred_label, filename):\n",
    "    \"\"\"\n",
    "    Plot attention weights as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        attention_matrix: (seq_len, seq_len) numpy array\n",
    "        sample_idx: sample identifier\n",
    "        true_label: ground truth label\n",
    "        pred_label: predicted label\n",
    "        filename: output filename\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.heatmap(attention_matrix, cmap='viridis', cbar=True)\n",
    "    plt.title(f'Attention Weights - Sample {sample_idx}\\nTrue: {true_label}, Pred: {pred_label}')\n",
    "    plt.xlabel('Key Position (Time Step)')\n",
    "    plt.ylabel('Query Position (Time Step)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Saved attention heatmap: {filename}\")\n",
    "\n",
    "\n",
    "def plot_integrated_gradients(attributions, sample_idx, true_label, pred_label, filename):\n",
    "    \"\"\"\n",
    "    Plot Integrated Gradients attribution scores.\n",
    "    \n",
    "    Args:\n",
    "        attributions: (T, feat_dim) numpy array\n",
    "        sample_idx: sample identifier\n",
    "        true_label: ground truth label\n",
    "        pred_label: predicted label\n",
    "        filename: output filename\n",
    "    \"\"\"\n",
    "    # Aggregate over feature dimension to get per-timestep importance\n",
    "    timestep_importance = np.abs(attributions).sum(axis=1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot 1: Per-timestep importance\n",
    "    ax1.plot(timestep_importance, linewidth=2)\n",
    "    ax1.set_title(f'Integrated Gradients - Sample {sample_idx}\\nTrue: {true_label}, Pred: {pred_label}')\n",
    "    ax1.set_xlabel('Time Step')\n",
    "    ax1.set_ylabel('Importance Score')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Full attribution heatmap (time x features)\n",
    "    im = ax2.imshow(attributions.T, aspect='auto', cmap='RdBu_r', \n",
    "                    vmin=-np.abs(attributions).max(), \n",
    "                    vmax=np.abs(attributions).max())\n",
    "    ax2.set_title('Feature-level Attributions')\n",
    "    ax2.set_xlabel('Time Step')\n",
    "    ax2.set_ylabel('Feature Dimension')\n",
    "    plt.colorbar(im, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Saved IG visualization: {filename}\")\n",
    "\n",
    "\n",
    "def plot_combined_analysis(attention_avg, ig_importance, sample_idx, true_label, pred_label, filename):\n",
    "    \"\"\"\n",
    "    Plot both attention and IG on the same time axis for comparison.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    \n",
    "    # Attention (average across all positions)\n",
    "    ax1.plot(attention_avg, linewidth=2, color='blue', label='Attention')\n",
    "    ax1.set_title(f'Explainability Analysis - Sample {sample_idx}\\nTrue: {true_label}, Pred: {pred_label}')\n",
    "    ax1.set_ylabel('Avg Attention Weight')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Integrated Gradients\n",
    "    ax2.plot(ig_importance, linewidth=2, color='red', label='Integrated Gradients')\n",
    "    ax2.set_xlabel('Time Step')\n",
    "    ax2.set_ylabel('IG Importance')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[INFO] Saved combined analysis: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da9e4fc0-aef7-48f1-a419-c2528cebabf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n",
      "[INFO] Loading model from: /scratch/jbfrantz/jess_explainability_model/jess_outputs/best_female_transformer.pt\n",
      "[INFO] Model loaded successfully\n",
      "[INFO] Total mel files found: 71\n",
      "[INFO] Usable (mel, label) pairs: 71\n",
      "[INFO] Patient-level split (files):\n",
      "       Train: 49\n",
      "       Val:   14\n",
      "       Test:  8\n",
      "\n",
      "[INFO] Loading test data...\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001157-100507.npy: 1272 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001163-100507.npy: 1572 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001198-100507.npy: 1599 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001285-100507.npy: 1502 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001320-100507.npy: 1922 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001380-100507.npy: 1577 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001069-100507.npy: 1513 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001171-100507.npy: 1258 segments, shape=(3, 64, 3001)\n",
      "[INFO] Total female segments: 12215\n",
      "[INFO] Found label classes: ['CentralApnea', 'CentralApnea;Hypopnea', 'CentralApnea;MixedApnea', 'CentralApnea;ObstructiveApnea', 'Hypopnea', 'Hypopnea;CentralApnea', 'Hypopnea;MixedApnea', 'Hypopnea;ObstructiveApnea', 'MixedApnea', 'MixedApnea;CentralApnea', 'MixedApnea;Hypopnea', 'MixedApnea;ObstructiveApnea', 'Normal', 'ObstructiveApnea', 'ObstructiveApnea;CentralApnea', 'ObstructiveApnea;Hypopnea', 'ObstructiveApnea;MixedApnea']\n",
      "[INFO] Binary label mapping:\n",
      "       Normal -> 0\n",
      "       CentralApnea -> 1 (event)\n",
      "       CentralApnea;Hypopnea -> 1 (event)\n",
      "       CentralApnea;MixedApnea -> 1 (event)\n",
      "       CentralApnea;ObstructiveApnea -> 1 (event)\n",
      "       Hypopnea -> 1 (event)\n",
      "       Hypopnea;CentralApnea -> 1 (event)\n",
      "       Hypopnea;MixedApnea -> 1 (event)\n",
      "       Hypopnea;ObstructiveApnea -> 1 (event)\n",
      "       MixedApnea -> 1 (event)\n",
      "       MixedApnea;CentralApnea -> 1 (event)\n",
      "       MixedApnea;Hypopnea -> 1 (event)\n",
      "       MixedApnea;ObstructiveApnea -> 1 (event)\n",
      "       ObstructiveApnea -> 1 (event)\n",
      "       ObstructiveApnea;CentralApnea -> 1 (event)\n",
      "       ObstructiveApnea;Hypopnea -> 1 (event)\n",
      "       ObstructiveApnea;MixedApnea -> 1 (event)\n",
      "\n",
      "[INFO] Running explainability on 20 samples...\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_12214.png\n",
      "\n",
      "[INFO] Explainability analysis complete!\n",
      "[INFO] Results saved to: /scratch/jbfrantz/jess_explainability_model/explainability_outputs\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Main Analysis\n",
    "# ==========================\n",
    "# def main():\n",
    "#     print(f\"[INFO] Using device: {DEVICE}\")\n",
    "#     print(f\"[INFO] Loading model from: {MODEL_PATH}\")\n",
    "    \n",
    "#     # Load model\n",
    "#     # Note: You'll need to know the input_dim from your training\n",
    "#     # You can save this info during training or infer it from test data\n",
    "    \n",
    "#     # For now, assuming you saved metadata\n",
    "#     metadata = np.load(METADATA_PATH.replace('best_female_transformer.pt', 'dataset_metadata.npz'))\n",
    "#     feat_dim = int(metadata['feature_dim'])\n",
    "    \n",
    "#     model = TinyMelTransformerWithAttention(\n",
    "#         input_dim=feat_dim,\n",
    "#         d_model=64,\n",
    "#         nhead=4,\n",
    "#         num_layers=2,\n",
    "#         dim_feedforward=128,\n",
    "#         dropout=0.1,\n",
    "#     ).to(DEVICE)\n",
    "    \n",
    "#     # Load trained weights\n",
    "#     state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    \n",
    "#     # Handle potential key mismatches (transformer vs encoder_layers)\n",
    "#     new_state_dict = {}\n",
    "#     for k, v in state_dict.items():\n",
    "#         if k.startswith('transformer.layers'):\n",
    "#             # Rename transformer.layers.X to encoder_layers.X\n",
    "#             new_key = k.replace('transformer.layers', 'encoder_layers')\n",
    "#             new_state_dict[new_key] = v\n",
    "#         else:\n",
    "#             new_state_dict[k] = v\n",
    "    \n",
    "#     model.load_state_dict(new_state_dict)\n",
    "#     model.eval()\n",
    "#     print(\"[INFO] Model loaded successfully\")\n",
    "\n",
    "#     pairs_all = find_file_pairs(MEL_DIR, LABEL_DIR)\n",
    "#     train_pairs, val_pairs, test_pairs = split_file_pairs(pairs_all, seed=42)\n",
    "    \n",
    "#     # Load test data (you'll need to recreate your test dataset)\n",
    "#     # This is just a placeholder - adjust based on your actual data loading\n",
    "#     print(\"\\n[INFO] Loading test data...\")\n",
    "#     # test_dataset = MelTransformerDataset(test_pairs, time_pool=10)\n",
    "#     # test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "#     test_dataset = MelTransformerDataset(test_pairs, time_pool=10)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "#     print(f\"\\n[INFO] Running explainability on {NUM_SAMPLES_TO_EXPLAIN} samples...\")\n",
    "    \n",
    "#     # Analyze samples\n",
    "#     sample_count = 0\n",
    "    \n",
    "#     # Placeholder for actual data loading\n",
    "#     for idx, (x, y) in enumerate(test_loader):\n",
    "#         if sample_count >= NUM_SAMPLES_TO_EXPLAIN:\n",
    "#             break\n",
    "    \n",
    "#     # Example analysis (you'll replace this with actual loop)\n",
    "#     x = x.to(DEVICE)  # (1, T, feat_dim)\n",
    "#     y = y.item()\n",
    "    \n",
    "#     ## Get prediction\n",
    "#     with torch.no_grad():\n",
    "#         logits, _ = model(x)\n",
    "#         pred_prob = torch.sigmoid(logits).item()\n",
    "#         pred_label = int(pred_prob >= 0.5)\n",
    "    \n",
    "#     # 1. Integrated Gradients\n",
    "#     ig_attributions = integrated_gradients(model, x, target_class=0, steps=50)\n",
    "#     ig_attr_np = ig_attributions.cpu().numpy()\n",
    "    \n",
    "#     # 2. Visualize\n",
    "#     ig_importance = np.abs(ig_attr_np).sum(axis=1)\n",
    "    \n",
    "#     plot_integrated_gradients(\n",
    "#         ig_attr_np, \n",
    "#         sample_idx=idx,\n",
    "#         true_label=y,\n",
    "#         pred_label=pred_label,\n",
    "#         filename=os.path.join(OUT_DIR, f'ig_sample_{idx}.png')\n",
    "#     )\n",
    "    \n",
    "#     sample_count += 1\n",
    "    \n",
    "#     print(\"\\n[INFO] Explainability analysis complete!\")\n",
    "#     print(f\"[INFO] Results saved to: {OUT_DIR}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e8a94a-71ad-4a51-85f2-25a9698e71e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n",
      "[INFO] Loading model from: /scratch/jbfrantz/jess_explainability_model/jess_outputs/best_female_transformer.pt\n",
      "[INFO] Model loaded successfully\n",
      "[INFO] Total mel files found: 71\n",
      "[INFO] Usable (mel, label) pairs: 71\n",
      "[INFO] Patient-level split (files):\n",
      "       Train: 49\n",
      "       Val:   14\n",
      "       Test:  8\n",
      "\n",
      "[INFO] Loading test data...\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001157-100507.npy: 1272 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001163-100507.npy: 1572 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001198-100507.npy: 1599 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001285-100507.npy: 1502 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001320-100507.npy: 1922 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001380-100507.npy: 1577 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001069-100507.npy: 1513 segments, shape=(3, 64, 3001)\n",
      "[INFO] Processed /scratch/sshuvo13/project_shared_folder_bspml_1/segments_30s/features/female/mel_spectrum/00001171-100507.npy: 1258 segments, shape=(3, 64, 3001)\n",
      "[INFO] Total female segments: 12215\n",
      "[INFO] Found label classes: ['CentralApnea', 'CentralApnea;Hypopnea', 'CentralApnea;MixedApnea', 'CentralApnea;ObstructiveApnea', 'Hypopnea', 'Hypopnea;CentralApnea', 'Hypopnea;MixedApnea', 'Hypopnea;ObstructiveApnea', 'MixedApnea', 'MixedApnea;CentralApnea', 'MixedApnea;Hypopnea', 'MixedApnea;ObstructiveApnea', 'Normal', 'ObstructiveApnea', 'ObstructiveApnea;CentralApnea', 'ObstructiveApnea;Hypopnea', 'ObstructiveApnea;MixedApnea']\n",
      "[INFO] Binary label mapping:\n",
      "       Normal -> 0\n",
      "       CentralApnea -> 1 (event)\n",
      "       CentralApnea;Hypopnea -> 1 (event)\n",
      "       CentralApnea;MixedApnea -> 1 (event)\n",
      "       CentralApnea;ObstructiveApnea -> 1 (event)\n",
      "       Hypopnea -> 1 (event)\n",
      "       Hypopnea;CentralApnea -> 1 (event)\n",
      "       Hypopnea;MixedApnea -> 1 (event)\n",
      "       Hypopnea;ObstructiveApnea -> 1 (event)\n",
      "       MixedApnea -> 1 (event)\n",
      "       MixedApnea;CentralApnea -> 1 (event)\n",
      "       MixedApnea;Hypopnea -> 1 (event)\n",
      "       MixedApnea;ObstructiveApnea -> 1 (event)\n",
      "       ObstructiveApnea -> 1 (event)\n",
      "       ObstructiveApnea;CentralApnea -> 1 (event)\n",
      "       ObstructiveApnea;Hypopnea -> 1 (event)\n",
      "       ObstructiveApnea;MixedApnea -> 1 (event)\n",
      "\n",
      "[INFO] Running explainability on 20 samples...\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_0.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_1.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_2.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_3.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_4.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_5.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_6.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_7.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_8.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_9.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_10.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_11.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_12.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_13.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_14.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_15.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_16.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_17.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_18.png\n",
      "[INFO] Saved IG visualization: /scratch/jbfrantz/jess_explainability_model/explainability_outputs/ig_sample_19.png\n",
      "\n",
      "[INFO] Explainability analysis complete!\n",
      "[INFO] Results saved to: /scratch/jbfrantz/jess_explainability_model/explainability_outputs\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(f\"[INFO] Using device: {DEVICE}\")\n",
    "    print(f\"[INFO] Loading model from: {MODEL_PATH}\")\n",
    "    \n",
    "    # Load model metadata\n",
    "    metadata = np.load(METADATA_PATH)\n",
    "    feat_dim = int(metadata['feature_dim'])\n",
    "    \n",
    "    model = TinyMelTransformerWithAttention(\n",
    "        input_dim=feat_dim,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Load trained weights\n",
    "    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    \n",
    "    # Handle potential key mismatches (transformer vs encoder_layers)\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('transformer.layers'):\n",
    "            new_key = k.replace('transformer.layers', 'encoder_layers')\n",
    "            new_state_dict[new_key] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    print(\"[INFO] Model loaded successfully\")\n",
    "    \n",
    "    # Load test data\n",
    "    pairs_all = find_file_pairs(MEL_DIR, LABEL_DIR)\n",
    "    train_pairs, val_pairs, test_pairs = split_file_pairs(pairs_all, seed=42)\n",
    "    \n",
    "    print(\"\\n[INFO] Loading test data...\")\n",
    "    test_dataset = MelTransformerDataset(test_pairs, time_pool=10)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    print(f\"\\n[INFO] Running explainability on {NUM_SAMPLES_TO_EXPLAIN} samples...\")\n",
    "    \n",
    "    # Analyze samples\n",
    "    sample_count = 0\n",
    "    \n",
    "    for idx, (x, y) in enumerate(test_loader):\n",
    "        if sample_count >= NUM_SAMPLES_TO_EXPLAIN:\n",
    "            break\n",
    "        \n",
    "        # Move to device\n",
    "        x = x.to(DEVICE)  # (1, T, feat_dim)\n",
    "        y = y.item()\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(x)\n",
    "            pred_prob = torch.sigmoid(logits).item()\n",
    "            pred_label = int(pred_prob >= 0.5)\n",
    "        \n",
    "        # 1. Integrated Gradients\n",
    "        ig_attributions = integrated_gradients(model, x, target_class=0, steps=50)\n",
    "        ig_attr_np = ig_attributions.cpu().numpy()\n",
    "        \n",
    "        # 2. Visualize\n",
    "        ig_importance = np.abs(ig_attr_np).sum(axis=1)\n",
    "        \n",
    "        plot_integrated_gradients(\n",
    "            ig_attr_np, \n",
    "            sample_idx=idx,\n",
    "            true_label=y,\n",
    "            pred_label=pred_label,\n",
    "            filename=os.path.join(OUT_DIR, f'ig_sample_{idx}.png')\n",
    "        )\n",
    "        \n",
    "        sample_count += 1\n",
    "    \n",
    "    print(\"\\n[INFO] Explainability analysis complete!\")\n",
    "    print(f\"[INFO] Results saved to: {OUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9a129-ffdb-4253-aade-23b211eccad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77158e7-5a49-47df-8971-9ce7843b42fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2ce1d-c713-4a10-9592-fe92f4c5bb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7a19c-a5ed-4a1d-b703-e935fdb467a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.0-cuda-12.1",
   "language": "python",
   "name": "pytorch-gpu-2.1.0-cuda-12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
