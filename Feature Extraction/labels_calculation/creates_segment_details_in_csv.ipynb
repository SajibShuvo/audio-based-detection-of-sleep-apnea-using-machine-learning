{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3a8a1c9-4818-449d-8154-0a9b212af27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Any\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- assume detect_event_in_window is implemented elsewhere and imported ---\n",
    "# from my_events import detect_event_in_window\n",
    "\n",
    "def _coerce_to_number(x: Any, name: str) -> float:\n",
    "    \"\"\"\n",
    "    Try to coerce x into a float. If x is a 1-element tuple/list, take its first element.\n",
    "    Raise TypeError with a helpful message if coercion fails.\n",
    "    \"\"\"\n",
    "    # if it's a list/tuple of length 1, unpack\n",
    "    if isinstance(x, (tuple, list)) and len(x) == 1:\n",
    "        x = x[0]\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception as e:\n",
    "        raise TypeError(f\"Parameter '{name}' must be a number (or single-element list/tuple). \"\n",
    "                        f\"Got {type(x).__name__}({x!r}). Error: {e}\")\n",
    "\n",
    "def _find_identifier_csv(directory: Path, identifier: str) -> Optional[Path]:\n",
    "    directory = Path(directory)\n",
    "    if not directory.exists() or not directory.is_dir():\n",
    "        return None\n",
    "    csv_files = sorted(directory.glob(\"*.csv\"))\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    # matching strategies\n",
    "    for p in csv_files:\n",
    "        if p.stem == identifier:\n",
    "            return p\n",
    "    for p in csv_files:\n",
    "        if identifier in p.stem:\n",
    "            return p\n",
    "    for p in csv_files:\n",
    "        if p.name == identifier or p.name == f\"{identifier}.csv\":\n",
    "            return p\n",
    "    for p in csv_files:\n",
    "        if identifier in p.name:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _lookup_total_duration_from_table(total_csv_path: Path, identifier: str) -> float:\n",
    "    total_csv_path = Path(total_csv_path)\n",
    "    if not total_csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Total durations CSV not found: {total_csv_path}\")\n",
    "    table = pd.read_csv(total_csv_path)\n",
    "    cols_lower = {c.lower(): c for c in table.columns}\n",
    "    id_candidates = [\"identifier\", \"id\", \"filename\", \"file\", \"subject\", \"recording\"]\n",
    "    dur_candidates = [\"total_duration\", \"duration\", \"recording_duration\", \"total_seconds\", \"total_time\"]\n",
    "    id_col = None\n",
    "    dur_col = None\n",
    "    for cand in id_candidates:\n",
    "        if cand in cols_lower:\n",
    "            id_col = cols_lower[cand]\n",
    "            break\n",
    "    for cand in dur_candidates:\n",
    "        if cand in cols_lower:\n",
    "            dur_col = cols_lower[cand]\n",
    "            break\n",
    "    if id_col is None or dur_col is None:\n",
    "        if table.shape[1] >= 2:\n",
    "            id_col = id_col or table.columns[0]\n",
    "            dur_col = dur_col or table.columns[1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not determine identifier and duration columns in {total_csv_path}. \"\n",
    "                \"Expect columns like 'identifier' and 'total_duration' or at least two columns.\"\n",
    "            )\n",
    "    table[id_col] = table[id_col].astype(str)\n",
    "    table[dur_col] = pd.to_numeric(table[dur_col], errors=\"coerce\")\n",
    "    match = table[table[id_col].str.strip() == identifier]\n",
    "    if match.empty:\n",
    "        match = table[table[id_col].str.replace(r\"\\.csv$\", \"\", regex=True).str.strip() == identifier]\n",
    "    if match.empty:\n",
    "        match = table[table[id_col].str.contains(identifier, na=False)]\n",
    "    if match.empty:\n",
    "        raise FileNotFoundError(f\"Identifier '{identifier}' not found in {total_csv_path}\")\n",
    "    total_val = match.iloc[0][dur_col]\n",
    "    if pd.isna(total_val):\n",
    "        raise ValueError(f\"Total duration for identifier '{identifier}' is NaN in {total_csv_path}\")\n",
    "    return float(total_val)\n",
    "\n",
    "def create_segment_csv_from_identifier(\n",
    "    identifier: str,\n",
    "    *,\n",
    "    events_dir: str = \"identified_apnea_events_in_csv\",\n",
    "    total_durations_csv: str = \"total_duration_of_each_patient_record.csv\",\n",
    "    segment_length: Any = 3600.0,\n",
    "    overlap: Any = 0.0,\n",
    "    output_path: Optional[str] = None,\n",
    "    join_multiple_events_with: str = \";\",\n",
    "    min_segment_duration: Any = 0.0,\n",
    ") -> Tuple[Path, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create a CSV with fixed-length (possibly overlapping) segments for the recording identified by `identifier`.\n",
    "\n",
    "    - segment_length, overlap and min_segment_duration are coerced to floats (accept single-element tuples/lists).\n",
    "    - overlap must satisfy 0 <= overlap < segment_length.\n",
    "    \"\"\"\n",
    "    # Coerce numeric params (handles e.g. tuple/list coming from widgets)\n",
    "    seg_len = _coerce_to_number(segment_length, \"segment_length\")\n",
    "    ovl = _coerce_to_number(overlap, \"overlap\")\n",
    "    min_seg_dur = _coerce_to_number(min_segment_duration, \"min_segment_duration\")\n",
    "\n",
    "    # Validate numeric params\n",
    "    if seg_len <= 0:\n",
    "        raise ValueError(\"segment_length must be > 0\")\n",
    "    if ovl < 0 or ovl >= seg_len:\n",
    "        raise ValueError(\"overlap must satisfy 0 <= overlap < segment_length\")\n",
    "\n",
    "    events_dir = Path(events_dir)\n",
    "    match = _find_identifier_csv(events_dir, identifier)\n",
    "    if match is None:\n",
    "        raise FileNotFoundError(f\"No matching events CSV found for identifier '{identifier}' in {events_dir}\")\n",
    "\n",
    "    # read events CSV and normalize column names\n",
    "    df = pd.read_csv(match)\n",
    "    df_cols_lower = {c.lower(): c for c in df.columns}\n",
    "    required = {\"start_time\", \"duration\", \"event_name\"}\n",
    "    if not required.issubset(set(df_cols_lower.keys())):\n",
    "        raise ValueError(f\"Events CSV must contain columns {required}. Found: {list(df.columns)}\")\n",
    "    df = df.rename(columns={\n",
    "        df_cols_lower[\"start_time\"]: \"start_time\",\n",
    "        df_cols_lower[\"duration\"]: \"duration\",\n",
    "        df_cols_lower[\"event_name\"]: \"event_name\"\n",
    "    })\n",
    "\n",
    "    total_csv_path = Path(total_durations_csv)\n",
    "    total_duration = _lookup_total_duration_from_table(total_csv_path, identifier)\n",
    "    if total_duration <= 0:\n",
    "        raise ValueError(f\"total_duration for identifier '{identifier}' must be > 0. Got {total_duration}\")\n",
    "\n",
    "    step = seg_len - ovl\n",
    "    if step <= 0:\n",
    "        raise ValueError(\"segment_length - overlap must be > 0 (positive step required)\")\n",
    "\n",
    "    n_segments = math.ceil((total_duration - ovl) / step)\n",
    "\n",
    "    rows = []\n",
    "    for seg_idx in range(n_segments):\n",
    "        start_time = seg_idx * step\n",
    "        end_time = start_time + seg_len\n",
    "        if end_time > total_duration:\n",
    "            end_time = total_duration\n",
    "\n",
    "        # Merge final too-short segment if requested\n",
    "        if seg_idx == n_segments - 1 and (end_time - start_time) < min_seg_dur and rows:\n",
    "            prev = rows[-1]\n",
    "            prev[\"end_time\"] = end_time\n",
    "            events_list = detect_event_in_window(df, prev[\"start_time\"], prev[\"end_time\"])\n",
    "            if isinstance(events_list, (list, tuple)):\n",
    "                prev[\"event_name\"] = join_multiple_events_with.join(events_list) if events_list else \"Normal\"\n",
    "            else:\n",
    "                prev[\"event_name\"] = str(events_list)\n",
    "            continue\n",
    "\n",
    "        events_list = detect_event_in_window(df, float(start_time), float(end_time))\n",
    "        if isinstance(events_list, (list, tuple)):\n",
    "            event_name = join_multiple_events_with.join(events_list) if events_list else \"Normal\"\n",
    "        else:\n",
    "            event_name = str(events_list)\n",
    "\n",
    "        rows.append({\n",
    "            \"segment_number\": seg_idx,\n",
    "            \"start_time\": float(start_time),\n",
    "            \"end_time\": float(end_time),\n",
    "            \"event_name\": event_name\n",
    "        })\n",
    "\n",
    "        if end_time >= total_duration:\n",
    "            break\n",
    "\n",
    "    seg_df = pd.DataFrame(rows, columns=[\"segment_number\", \"start_time\", \"end_time\", \"event_name\"])\n",
    "    # out_path = Path(output_path) if output_path else Path(f\"{identifier}_segments.csv\")\n",
    "    out_path = output_path / Path(f\"{identifier}_segments.csv\")\n",
    "    seg_df.to_csv(out_path, index=False)\n",
    "\n",
    "    return out_path, seg_df\n",
    "\n",
    "# --------------------\n",
    "# Example usage:\n",
    "# --------------------\n",
    "# out_path, segs = create_segment_csv_from_identifier(\n",
    "#     \"subject_001\",\n",
    "#     events_dir=\"identified_apnea_events_in_csv\",\n",
    "#     total_durations_csv=\"total_duration_of_each_patient_record.csv\",\n",
    "#     segment_length=(3600.0,),  # now accepts single-element tuple too\n",
    "#     overlap=(60.0,),           # single-element tuple/list will be coerced\n",
    "#     output_path=None\n",
    "# )\n",
    "# print(\"Wrote:\", out_path)\n",
    "# print(segs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c51bc61-c9c3-4bdb-bd26-8b8953cc2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_event_in_window(df: pd.DataFrame, window_start: float, window_end: float):\n",
    "    \"\"\"\n",
    "    Detects which event(s) occur within a specified time window.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns ['start_time', 'duration', 'event_name'].\n",
    "        - start_time: event start time in seconds\n",
    "        - duration: event duration in seconds\n",
    "        - event_name: name/label of the event\n",
    "    window_start : float\n",
    "        Start time of the window (in seconds).\n",
    "    window_end : float\n",
    "        End time of the window (in seconds).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of event names that overlap with the given time window.\n",
    "        Returns an [\"Normal\"] if no overlap is found.\n",
    "    \"\"\"\n",
    "    # Compute end time of each event\n",
    "    df = df.copy()\n",
    "    df[\"end_time\"] = df[\"start_time\"] + df[\"duration\"]\n",
    "\n",
    "    # Find overlapping events: event overlaps if its range intersects the window\n",
    "    overlap_mask = (df[\"start_time\"] < window_end) & (df[\"end_time\"] > window_start)\n",
    "    overlapping_events = df.loc[overlap_mask, \"event_name\"].unique().tolist()\n",
    "    if not overlapping_events:\n",
    "        return [\"Normal\"]\n",
    "    else:\n",
    "        return overlapping_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24f3274e-465d-4adb-9914-757b5c1a6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # identifier = \"00000995-100507\"\n",
    "# events_dir = \"/home/sshuvo13/BSPML_project_sbs_files/segmentation_30s/labels_again/new_apnea_events_identified_in_csv\"\n",
    "# total_durations_csv=\"/home/sshuvo13/BSPML_project_sbs_files/segmentation_30s/total_duration_of_each_patient_record.csv\"\n",
    "# segment_length=3600.0,\n",
    "# overlap=100    # 60 seconds overlap\n",
    "# output_path=\"./rml_outputs/segment_details\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf022e6a-b299-433a-b21d-d640aa0f1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_path, segs = create_segment_csv_from_identifier(\n",
    "#     identifier,\n",
    "#     events_dir=events_dir,\n",
    "#     total_durations_csv=total_durations_csv,\n",
    "#     segment_length=segment_length,\n",
    "#     overlap=overlap,    # 60 seconds overlap\n",
    "#     output_path=output_path\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33683e8a-15c6-4458-b85a-98c732dd8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Any\n",
    "import pandas as pd\n",
    "\n",
    "# --- assume detect_event_in_window is available in this namespace ---\n",
    "\n",
    "\n",
    "def _coerce_to_number(x: Any, name: str) -> float:\n",
    "    if isinstance(x, (tuple, list)) and len(x) == 1:\n",
    "        x = x[0]\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception as e:\n",
    "        raise TypeError(f\"Parameter '{name}' must be a number (or single-element list/tuple). Got {type(x).__name__}({x!r}). Error: {e}\")\n",
    "\n",
    "\n",
    "def _find_identifier_csv(directory: Path, identifier: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Find a csv in directory matching identifier. Return Path or None.\n",
    "    \"\"\"\n",
    "    directory = Path(directory)\n",
    "    if not directory.exists() or not directory.is_dir():\n",
    "        return None\n",
    "    csv_files = sorted(directory.glob(\"*.csv\"))\n",
    "    if not csv_files:\n",
    "        return None\n",
    "\n",
    "    # try exact stem, contains, full name\n",
    "    for p in csv_files:\n",
    "        if p.stem == identifier:\n",
    "            return p\n",
    "    for p in csv_files:\n",
    "        if identifier in p.stem:\n",
    "            return p\n",
    "    for p in csv_files:\n",
    "        if p.name == identifier or p.name == f\"{identifier}.csv\":\n",
    "            return p\n",
    "    for p in csv_files:\n",
    "        if identifier in p.name:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "def _lookup_total_duration_from_table(total_csv_path: Path, identifier: str) -> float:\n",
    "    \"\"\"\n",
    "    Same as before: find total_duration for identifier from totals CSV.\n",
    "    \"\"\"\n",
    "    total_csv_path = Path(total_csv_path)\n",
    "    if not total_csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Total durations CSV not found: {total_csv_path}\")\n",
    "    table = pd.read_csv(total_csv_path)\n",
    "    cols_lower = {c.lower(): c for c in table.columns}\n",
    "    id_candidates = [\"identifier\", \"id\", \"filename\", \"file\", \"subject\", \"recording\"]\n",
    "    dur_candidates = [\"total_duration\", \"duration\", \"recording_duration\", \"total_seconds\", \"total_time\"]\n",
    "    id_col = None\n",
    "    dur_col = None\n",
    "    for cand in id_candidates:\n",
    "        if cand in cols_lower:\n",
    "            id_col = cols_lower[cand]\n",
    "            break\n",
    "    for cand in dur_candidates:\n",
    "        if cand in cols_lower:\n",
    "            dur_col = cols_lower[cand]\n",
    "            break\n",
    "    if id_col is None or dur_col is None:\n",
    "        if table.shape[1] >= 2:\n",
    "            id_col = id_col or table.columns[0]\n",
    "            dur_col = dur_col or table.columns[1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Could not determine identifier and duration columns in {total_csv_path}. \"\n",
    "                \"Expect columns like 'identifier' and 'total_duration' or at least two columns.\"\n",
    "            )\n",
    "    table[id_col] = table[id_col].astype(str)\n",
    "    table[dur_col] = pd.to_numeric(table[dur_col], errors=\"coerce\")\n",
    "    match = table[table[id_col].str.strip() == identifier]\n",
    "    if match.empty:\n",
    "        match = table[table[id_col].str.replace(r\"\\.csv$\", \"\", regex=True).str.strip() == identifier]\n",
    "    if match.empty:\n",
    "        match = table[table[id_col].str.contains(identifier, na=False)]\n",
    "    if match.empty:\n",
    "        raise FileNotFoundError(f\"Identifier '{identifier}' not found in {total_csv_path}\")\n",
    "    total_val = match.iloc[0][dur_col]\n",
    "    if pd.isna(total_val):\n",
    "        raise ValueError(f\"Total duration for identifier '{identifier}' is NaN in {total_csv_path}\")\n",
    "    return float(total_val)\n",
    "\n",
    "\n",
    "def create_segment_csv_from_identifier(\n",
    "    identifier: str,\n",
    "    *,\n",
    "    events_dir: str = \"identified_apnea_events_in_csv\",\n",
    "    total_durations_csv: str = \"total_duration_of_each_patient_record.csv\",\n",
    "    segment_length: Any = 3600.0,\n",
    "    overlap: Any = 0.0,\n",
    "    output_path: Optional[str] = None,\n",
    "    join_multiple_events_with: str = \";\",\n",
    "    min_segment_duration: Any = 0.0,\n",
    ") -> Tuple[Path, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create segment CSV for identifier. Defensive: tries sanitized identifier fallback,\n",
    "    ensures output directory exists before writing, and raises clear messages on failure.\n",
    "    \"\"\"\n",
    "    seg_len = _coerce_to_number(segment_length, \"segment_length\")\n",
    "    ovl = _coerce_to_number(overlap, \"overlap\")\n",
    "    min_seg_dur = _coerce_to_number(min_segment_duration, \"min_segment_duration\")\n",
    "\n",
    "    if seg_len <= 0:\n",
    "        raise ValueError(\"segment_length must be > 0\")\n",
    "    if ovl < 0 or ovl >= seg_len:\n",
    "        raise ValueError(\"overlap must satisfy 0 <= overlap < segment_length\")\n",
    "\n",
    "    events_dir_p = Path(events_dir)\n",
    "    # try exact identifier first\n",
    "    match = _find_identifier_csv(events_dir_p, identifier)\n",
    "\n",
    "    # sanitization fallback: strip common suffixes (e.g., \"-checkpoint\") and retry\n",
    "    if match is None and isinstance(identifier, str) and identifier.endswith(\"-checkpoint\"):\n",
    "        cleaned = identifier.replace(\"-checkpoint\", \"\")\n",
    "        match = _find_identifier_csv(events_dir_p, cleaned)\n",
    "\n",
    "    if match is None:\n",
    "        raise FileNotFoundError(f\"No matching events CSV found for identifier '{identifier}' in {events_dir_p}\")\n",
    "\n",
    "    # read events dataframe and normalize\n",
    "    df = pd.read_csv(match)\n",
    "    df_cols_lower = {c.lower(): c for c in df.columns}\n",
    "    required = {\"start_time\", \"duration\", \"event_name\"}\n",
    "    if not required.issubset(set(df_cols_lower.keys())):\n",
    "        raise ValueError(f\"Events CSV must contain columns {required}. Found: {list(df.columns)}\")\n",
    "    df = df.rename(columns={\n",
    "        df_cols_lower[\"start_time\"]: \"start_time\",\n",
    "        df_cols_lower[\"duration\"]: \"duration\",\n",
    "        df_cols_lower[\"event_name\"]: \"event_name\"\n",
    "    })\n",
    "\n",
    "    total_csv_path = Path(total_durations_csv)\n",
    "    total_duration = _lookup_total_duration_from_table(total_csv_path, identifier)\n",
    "\n",
    "    if total_duration <= 0:\n",
    "        raise ValueError(f\"total_duration for identifier '{identifier}' must be > 0. Got {total_duration}\")\n",
    "\n",
    "    step = seg_len - ovl\n",
    "    if step <= 0:\n",
    "        raise ValueError(\"segment_length - overlap must be > 0 (positive step required)\")\n",
    "\n",
    "    n_segments = math.ceil((total_duration - ovl) / step)\n",
    "    rows = []\n",
    "    for seg_idx in range(n_segments):\n",
    "        start_time = seg_idx * step\n",
    "        end_time = min(start_time + seg_len, total_duration)\n",
    "\n",
    "        # merge too-short final segment into previous if requested\n",
    "        if seg_idx == n_segments - 1 and (end_time - start_time) < min_seg_dur and rows:\n",
    "            prev = rows[-1]\n",
    "            prev[\"end_time\"] = end_time\n",
    "            events_list = detect_event_in_window(df, prev[\"start_time\"], prev[\"end_time\"])\n",
    "            if isinstance(events_list, (list, tuple)):\n",
    "                prev[\"event_name\"] = join_multiple_events_with.join(events_list) if events_list else \"Normal\"\n",
    "            else:\n",
    "                prev[\"event_name\"] = str(events_list)\n",
    "            continue\n",
    "\n",
    "        events_list = detect_event_in_window(df, float(start_time), float(end_time))\n",
    "        if isinstance(events_list, (list, tuple)):\n",
    "            event_name = join_multiple_events_with.join(events_list) if events_list else \"Normal\"\n",
    "        else:\n",
    "            event_name = str(events_list)\n",
    "\n",
    "        rows.append({\n",
    "            \"segment_number\": seg_idx,\n",
    "            \"start_time\": float(start_time),\n",
    "            \"end_time\": float(end_time),\n",
    "            \"event_name\": event_name\n",
    "        })\n",
    "\n",
    "        if end_time >= total_duration:\n",
    "            break\n",
    "\n",
    "    seg_df = pd.DataFrame(rows, columns=[\"segment_number\", \"start_time\", \"end_time\", \"event_name\"])\n",
    "\n",
    "    # Prepare output path and ensure parent exists BEFORE writing\n",
    "    out_path = Path(output_path) if output_path else Path(f\"{identifier}_segments.csv\")\n",
    "    try:\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        raise OSError(f\"Cannot create output directory '{out_path.parent}': {e}\") from e\n",
    "\n",
    "    try:\n",
    "        seg_df.to_csv(out_path, index=False)\n",
    "    except Exception as e:\n",
    "        # Raise a clearer error so calling code can log it neatly\n",
    "        raise OSError(f\"Failed to write segments CSV to '{out_path}': {e}\") from e\n",
    "\n",
    "    return out_path, seg_df\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Batch runner (update to call/create parent dir)\n",
    "# ----------------------\n",
    "def batch_create_segments(\n",
    "    totals_csv: str,\n",
    "    output_dir: str = \"segments_output\",\n",
    "    events_dir: str = \"identified_apnea_events_in_csv\",\n",
    "    segment_length: float = 3600.0,\n",
    "    overlap: float = 0.0,\n",
    "    total_durations_csv: str = \"total_duration_of_each_patient_record.csv\",\n",
    "    join_multiple_events_with: str = \";\",\n",
    "    min_segment_duration: float = 0.0,\n",
    "):\n",
    "    totals_csv = Path(totals_csv)\n",
    "    if not totals_csv.exists():\n",
    "        raise FileNotFoundError(f\"Totals CSV not found: {totals_csv}\")\n",
    "\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)  # ensure base output dir exists\n",
    "\n",
    "    # read identifiers (first column)\n",
    "    df_totals = pd.read_csv(totals_csv, dtype=str, keep_default_na=False)\n",
    "    if df_totals.shape[1] == 0:\n",
    "        print(\"No identifiers found in totals CSV.\")\n",
    "        return\n",
    "    first_col = df_totals.columns[0]\n",
    "    identifiers = df_totals[first_col].astype(str).str.strip().replace(\"\", pd.NA).dropna().unique().tolist()\n",
    "\n",
    "    success = []\n",
    "    failed = []\n",
    "    for ident in identifiers:\n",
    "        # generate per-identifier output path (can be nested)\n",
    "        target_outfile = out_dir / f\"{ident}_segments.csv\"\n",
    "        try:\n",
    "            # make sure the parent of this particular output file exists\n",
    "            target_outfile.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            out_path, seg_df = create_segment_csv_from_identifier(\n",
    "                identifier=ident,\n",
    "                events_dir=events_dir,\n",
    "                total_durations_csv=total_durations_csv,\n",
    "                segment_length=segment_length,\n",
    "                overlap=overlap,\n",
    "                output_path=str(target_outfile),\n",
    "                join_multiple_events_with=join_multiple_events_with,\n",
    "                min_segment_duration=min_segment_duration,\n",
    "            )\n",
    "            success.append((ident, out_path, len(seg_df)))\n",
    "            print(f\"[OK] {ident} -> {out_path} ({len(seg_df)} segments)\")\n",
    "        except Exception as e:\n",
    "            failed.append((ident, str(e)))\n",
    "            print(f\"[ERR] {ident} -> {e}\")\n",
    "\n",
    "    # summary\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Processed: {len(identifiers)}; success: {len(success)}; failed: {len(failed)}\")\n",
    "    if failed:\n",
    "        print(\"\\nFailures (sample):\")\n",
    "        for ident, msg in failed[:10]:\n",
    "            print(f\"- {ident}: {msg}\")\n",
    "\n",
    "    return success, failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a239a59a-6532-472b-b390-835a9c3190bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_csv = \"/home/sshuvo13/BSPML_project_sbs_files/segmentation_30s/total_duration_of_each_patient_record.csv\"\n",
    "events_dir = \"/home/sshuvo13/BSPML_project_sbs_files/segmentation_30s/labels_again/new_apnea_events_identified_in_csv\"\n",
    "output_path = \"./fixed_9s_segment_details_of_each_label/\"\n",
    "os.makedirs(output_path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6b7d991-25dc-4a36-b9df-fffb478e4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(\"./rml_outputs/segment_details\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc9858f6-e0d0-4708-93a3-d584276014d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 00000995-100507 -> fixed_9s_segment_of_each_label/00000995-100507_segments.csv (5962 segments)\n",
      "[OK] 00001006-100507 -> fixed_9s_segment_of_each_label/00001006-100507_segments.csv (4770 segments)\n",
      "[OK] 00001008-100507 -> fixed_9s_segment_of_each_label/00001008-100507_segments.csv (5044 segments)\n",
      "[OK] 00001010-100507 -> fixed_9s_segment_of_each_label/00001010-100507_segments.csv (4412 segments)\n",
      "[OK] 00001016-100507 -> fixed_9s_segment_of_each_label/00001016-100507_segments.csv (7280 segments)\n",
      "[OK] 00001018-100507 -> fixed_9s_segment_of_each_label/00001018-100507_segments.csv (5088 segments)\n",
      "[OK] 00001020-100507 -> fixed_9s_segment_of_each_label/00001020-100507_segments.csv (5504 segments)\n",
      "[OK] 00001028-100507 -> fixed_9s_segment_of_each_label/00001028-100507_segments.csv (7360 segments)\n",
      "[OK] 00001041-100507 -> fixed_9s_segment_of_each_label/00001041-100507_segments.csv (4998 segments)\n",
      "[OK] 00001043-100507 -> fixed_9s_segment_of_each_label/00001043-100507_segments.csv (5103 segments)\n",
      "[OK] 00001069-100507 -> fixed_9s_segment_of_each_label/00001069-100507_segments.csv (5048 segments)\n",
      "[OK] 00001073-100507 -> fixed_9s_segment_of_each_label/00001073-100507_segments.csv (4853 segments)\n",
      "[OK] 00001084-100507 -> fixed_9s_segment_of_each_label/00001084-100507_segments.csv (7233 segments)\n",
      "[OK] 00001086-100507 -> fixed_9s_segment_of_each_label/00001086-100507_segments.csv (5207 segments)\n",
      "[OK] 00001093-100507 -> fixed_9s_segment_of_each_label/00001093-100507_segments.csv (5376 segments)\n",
      "[OK] 00001097-100507 -> fixed_9s_segment_of_each_label/00001097-100507_segments.csv (6411 segments)\n",
      "[OK] 00001110-100507 -> fixed_9s_segment_of_each_label/00001110-100507_segments.csv (5002 segments)\n",
      "[OK] 00001118-100507 -> fixed_9s_segment_of_each_label/00001118-100507_segments.csv (4517 segments)\n",
      "[OK] 00001120-100507 -> fixed_9s_segment_of_each_label/00001120-100507_segments.csv (5657 segments)\n",
      "[OK] 00001126-100507 -> fixed_9s_segment_of_each_label/00001126-100507_segments.csv (5180 segments)\n",
      "[OK] 00001127-100507 -> fixed_9s_segment_of_each_label/00001127-100507_segments.csv (5902 segments)\n",
      "[OK] 00001131-100507 -> fixed_9s_segment_of_each_label/00001131-100507_segments.csv (4726 segments)\n",
      "[OK] 00001139-100507 -> fixed_9s_segment_of_each_label/00001139-100507_segments.csv (5196 segments)\n",
      "[OK] 00001143-100507 -> fixed_9s_segment_of_each_label/00001143-100507_segments.csv (5756 segments)\n",
      "[OK] 00001147-100507 -> fixed_9s_segment_of_each_label/00001147-100507_segments.csv (7692 segments)\n",
      "[OK] 00001153-100507 -> fixed_9s_segment_of_each_label/00001153-100507_segments.csv (5441 segments)\n",
      "[OK] 00001157-100507 -> fixed_9s_segment_of_each_label/00001157-100507_segments.csv (4244 segments)\n",
      "[OK] 00001161-100507 -> fixed_9s_segment_of_each_label/00001161-100507_segments.csv (5792 segments)\n",
      "[OK] 00001163-100507 -> fixed_9s_segment_of_each_label/00001163-100507_segments.csv (5243 segments)\n",
      "[OK] 00001171-100507 -> fixed_9s_segment_of_each_label/00001171-100507_segments.csv (4197 segments)\n",
      "[OK] 00001182-100507 -> fixed_9s_segment_of_each_label/00001182-100507_segments.csv (4669 segments)\n",
      "[OK] 00001186-100507 -> fixed_9s_segment_of_each_label/00001186-100507_segments.csv (4809 segments)\n",
      "[OK] 00001191-100507 -> fixed_9s_segment_of_each_label/00001191-100507_segments.csv (3970 segments)\n",
      "[OK] 00001198-100507 -> fixed_9s_segment_of_each_label/00001198-100507_segments.csv (5333 segments)\n",
      "[OK] 00001202-100507 -> fixed_9s_segment_of_each_label/00001202-100507_segments.csv (4385 segments)\n",
      "[OK] 00001206-100507 -> fixed_9s_segment_of_each_label/00001206-100507_segments.csv (5864 segments)\n",
      "[OK] 00001222-100507 -> fixed_9s_segment_of_each_label/00001222-100507_segments.csv (6481 segments)\n",
      "[OK] 00001224-100507 -> fixed_9s_segment_of_each_label/00001224-100507_segments.csv (5170 segments)\n",
      "[OK] 00001234-100507 -> fixed_9s_segment_of_each_label/00001234-100507_segments.csv (4895 segments)\n",
      "[OK] 00001245-100507 -> fixed_9s_segment_of_each_label/00001245-100507_segments.csv (4848 segments)\n",
      "[OK] 00001247-100507 -> fixed_9s_segment_of_each_label/00001247-100507_segments.csv (4877 segments)\n",
      "[OK] 00001249-100507 -> fixed_9s_segment_of_each_label/00001249-100507_segments.csv (5421 segments)\n",
      "[OK] 00001250-100507 -> fixed_9s_segment_of_each_label/00001250-100507_segments.csv (8760 segments)\n",
      "[OK] 00001254-100507 -> fixed_9s_segment_of_each_label/00001254-100507_segments.csv (5651 segments)\n",
      "[OK] 00001256-100507 -> fixed_9s_segment_of_each_label/00001256-100507_segments.csv (4904 segments)\n",
      "[OK] 00001258-100507 -> fixed_9s_segment_of_each_label/00001258-100507_segments.csv (6034 segments)\n",
      "[OK] 00001262-100507 -> fixed_9s_segment_of_each_label/00001262-100507_segments.csv (4221 segments)\n",
      "[OK] 00001263-100507 -> fixed_9s_segment_of_each_label/00001263-100507_segments.csv (6343 segments)\n",
      "[OK] 00001274-100507 -> fixed_9s_segment_of_each_label/00001274-100507_segments.csv (5936 segments)\n",
      "[OK] 00001276-100507 -> fixed_9s_segment_of_each_label/00001276-100507_segments.csv (6312 segments)\n",
      "[OK] 00001282-100507 -> fixed_9s_segment_of_each_label/00001282-100507_segments.csv (4622 segments)\n",
      "[OK] 00001285-100507 -> fixed_9s_segment_of_each_label/00001285-100507_segments.csv (5010 segments)\n",
      "[OK] 00001287-100507 -> fixed_9s_segment_of_each_label/00001287-100507_segments.csv (9807 segments)\n",
      "[OK] 00001295-100507 -> fixed_9s_segment_of_each_label/00001295-100507_segments.csv (4848 segments)\n",
      "[OK] 00001301-100507 -> fixed_9s_segment_of_each_label/00001301-100507_segments.csv (5888 segments)\n",
      "[OK] 00001306-100507 -> fixed_9s_segment_of_each_label/00001306-100507_segments.csv (6155 segments)\n",
      "[OK] 00001318-100507 -> fixed_9s_segment_of_each_label/00001318-100507_segments.csv (6260 segments)\n",
      "[OK] 00001320-100507 -> fixed_9s_segment_of_each_label/00001320-100507_segments.csv (6410 segments)\n",
      "[OK] 00001322-100507 -> fixed_9s_segment_of_each_label/00001322-100507_segments.csv (4771 segments)\n",
      "[OK] 00001324-100507 -> fixed_9s_segment_of_each_label/00001324-100507_segments.csv (4639 segments)\n",
      "[OK] 00001329-100507 -> fixed_9s_segment_of_each_label/00001329-100507_segments.csv (5320 segments)\n",
      "[OK] 00001331-100507 -> fixed_9s_segment_of_each_label/00001331-100507_segments.csv (4898 segments)\n",
      "[OK] 00001335-100507 -> fixed_9s_segment_of_each_label/00001335-100507_segments.csv (7206 segments)\n",
      "[OK] 00001342-100507 -> fixed_9s_segment_of_each_label/00001342-100507_segments.csv (4894 segments)\n",
      "[OK] 00001355-100507 -> fixed_9s_segment_of_each_label/00001355-100507_segments.csv (4908 segments)\n",
      "[OK] 00001357-100507 -> fixed_9s_segment_of_each_label/00001357-100507_segments.csv (3657 segments)\n",
      "[OK] 00001358-100507 -> fixed_9s_segment_of_each_label/00001358-100507_segments.csv (6644 segments)\n",
      "[OK] 00001367-100507 -> fixed_9s_segment_of_each_label/00001367-100507_segments.csv (5056 segments)\n",
      "[OK] 00001369-100507 -> fixed_9s_segment_of_each_label/00001369-100507_segments.csv (7271 segments)\n",
      "[OK] 00001380-100507 -> fixed_9s_segment_of_each_label/00001380-100507_segments.csv (5261 segments)\n",
      "[OK] 00001382-100507 -> fixed_9s_segment_of_each_label/00001382-100507_segments.csv (9165 segments)\n",
      "[OK] 00001388-100507 -> fixed_9s_segment_of_each_label/00001388-100507_segments.csv (5302 segments)\n",
      "[OK] 00001390-100507 -> fixed_9s_segment_of_each_label/00001390-100507_segments.csv (4921 segments)\n",
      "[OK] 00001392-100507 -> fixed_9s_segment_of_each_label/00001392-100507_segments.csv (6892 segments)\n",
      "[OK] 00001396-100507 -> fixed_9s_segment_of_each_label/00001396-100507_segments.csv (5193 segments)\n",
      "[OK] 00001398-100507 -> fixed_9s_segment_of_each_label/00001398-100507_segments.csv (6443 segments)\n",
      "[OK] 00001400-100507 -> fixed_9s_segment_of_each_label/00001400-100507_segments.csv (4991 segments)\n",
      "[OK] 00001408-100507 -> fixed_9s_segment_of_each_label/00001408-100507_segments.csv (4375 segments)\n",
      "[OK] 00001411-100507 -> fixed_9s_segment_of_each_label/00001411-100507_segments.csv (7616 segments)\n",
      "[OK] 00001412-100507 -> fixed_9s_segment_of_each_label/00001412-100507_segments.csv (4825 segments)\n",
      "[OK] 00001414-100507 -> fixed_9s_segment_of_each_label/00001414-100507_segments.csv (4646 segments)\n",
      "[OK] 00001419-100507 -> fixed_9s_segment_of_each_label/00001419-100507_segments.csv (6307 segments)\n",
      "[OK] 00001424-100507 -> fixed_9s_segment_of_each_label/00001424-100507_segments.csv (5574 segments)\n",
      "[OK] 00001432-100507 -> fixed_9s_segment_of_each_label/00001432-100507_segments.csv (5344 segments)\n",
      "[OK] 00001442-100507 -> fixed_9s_segment_of_each_label/00001442-100507_segments.csv (5255 segments)\n",
      "[OK] 00001444-100507 -> fixed_9s_segment_of_each_label/00001444-100507_segments.csv (3827 segments)\n",
      "[OK] 00001447-100507 -> fixed_9s_segment_of_each_label/00001447-100507_segments.csv (5502 segments)\n",
      "[OK] 00001449-100507 -> fixed_9s_segment_of_each_label/00001449-100507_segments.csv (4834 segments)\n",
      "[OK] 00001451-100507 -> fixed_9s_segment_of_each_label/00001451-100507_segments.csv (6047 segments)\n",
      "[OK] 00001457-100507 -> fixed_9s_segment_of_each_label/00001457-100507_segments.csv (4453 segments)\n",
      "[OK] 00001459-100507 -> fixed_9s_segment_of_each_label/00001459-100507_segments.csv (6619 segments)\n",
      "[OK] 00001463-100507 -> fixed_9s_segment_of_each_label/00001463-100507_segments.csv (4500 segments)\n",
      "[OK] 00001474-100507 -> fixed_9s_segment_of_each_label/00001474-100507_segments.csv (6405 segments)\n",
      "[OK] 00001478-100507 -> fixed_9s_segment_of_each_label/00001478-100507_segments.csv (4200 segments)\n",
      "[OK] 00001486-100507 -> fixed_9s_segment_of_each_label/00001486-100507_segments.csv (6648 segments)\n",
      "[OK] 00001488-100507 -> fixed_9s_segment_of_each_label/00001488-100507_segments.csv (7080 segments)\n",
      "[OK] 00001498-100507 -> fixed_9s_segment_of_each_label/00001498-100507_segments.csv (6807 segments)\n",
      "[OK] 00001500-100507 -> fixed_9s_segment_of_each_label/00001500-100507_segments.csv (7470 segments)\n",
      "[OK] 00001508-100507 -> fixed_9s_segment_of_each_label/00001508-100507_segments.csv (7040 segments)\n",
      "[OK] 00001510-100507 -> fixed_9s_segment_of_each_label/00001510-100507_segments.csv (3695 segments)\n",
      "[OK] 00001516-100507 -> fixed_9s_segment_of_each_label/00001516-100507_segments.csv (7017 segments)\n",
      "[OK] 00001518-100507 -> fixed_9s_segment_of_each_label/00001518-100507_segments.csv (6498 segments)\n",
      "[OK] 00001521-100507 -> fixed_9s_segment_of_each_label/00001521-100507_segments.csv (7007 segments)\n",
      "[OK] 00001549-100507 -> fixed_9s_segment_of_each_label/00001549-100507_segments.csv (3603 segments)\n",
      "[OK] 00001550-100507 -> fixed_9s_segment_of_each_label/00001550-100507_segments.csv (4637 segments)\n",
      "[OK] 00001554-100507 -> fixed_9s_segment_of_each_label/00001554-100507_segments.csv (7081 segments)\n",
      "[OK] 00001556-100507 -> fixed_9s_segment_of_each_label/00001556-100507_segments.csv (9346 segments)\n",
      "[OK] 00001558-100507 -> fixed_9s_segment_of_each_label/00001558-100507_segments.csv (7420 segments)\n",
      "[OK] 00001568-100507 -> fixed_9s_segment_of_each_label/00001568-100507_segments.csv (7063 segments)\n",
      "[OK] 00001571-100507 -> fixed_9s_segment_of_each_label/00001571-100507_segments.csv (3689 segments)\n",
      "[OK] 00001575-100507 -> fixed_9s_segment_of_each_label/00001575-100507_segments.csv (3864 segments)\n",
      "[OK] 00001576-100507 -> fixed_9s_segment_of_each_label/00001576-100507_segments.csv (4630 segments)\n",
      "[OK] 00001577-100507 -> fixed_9s_segment_of_each_label/00001577-100507_segments.csv (8524 segments)\n",
      "[OK] 00001579-100507 -> fixed_9s_segment_of_each_label/00001579-100507_segments.csv (8155 segments)\n",
      "[OK] 00001593-100507 -> fixed_9s_segment_of_each_label/00001593-100507_segments.csv (8660 segments)\n",
      "[OK] 00001594-100507 -> fixed_9s_segment_of_each_label/00001594-100507_segments.csv (7702 segments)\n",
      "[OK] 00001597-100507 -> fixed_9s_segment_of_each_label/00001597-100507_segments.csv (7898 segments)\n",
      "[OK] 00001606-100507 -> fixed_9s_segment_of_each_label/00001606-100507_segments.csv (4395 segments)\n",
      "[OK] 00001607-100507 -> fixed_9s_segment_of_each_label/00001607-100507_segments.csv (8671 segments)\n",
      "[OK] 00001611-100507 -> fixed_9s_segment_of_each_label/00001611-100507_segments.csv (8368 segments)\n",
      "[OK] 00001614-100507 -> fixed_9s_segment_of_each_label/00001614-100507_segments.csv (8758 segments)\n",
      "[OK] 00001615-100507 -> fixed_9s_segment_of_each_label/00001615-100507_segments.csv (8738 segments)\n",
      "[OK] 00001616-100507 -> fixed_9s_segment_of_each_label/00001616-100507_segments.csv (8570 segments)\n",
      "[OK] 00001619-100507 -> fixed_9s_segment_of_each_label/00001619-100507_segments.csv (8855 segments)\n",
      "[OK] 00001623-100507 -> fixed_9s_segment_of_each_label/00001623-100507_segments.csv (8198 segments)\n",
      "[OK] 00001626-100507 -> fixed_9s_segment_of_each_label/00001626-100507_segments.csv (8119 segments)\n",
      "[OK] 00001629-100507 -> fixed_9s_segment_of_each_label/00001629-100507_segments.csv (8934 segments)\n",
      "[OK] 00001636-100507 -> fixed_9s_segment_of_each_label/00001636-100507_segments.csv (4207 segments)\n",
      "[OK] 00001640-100507 -> fixed_9s_segment_of_each_label/00001640-100507_segments.csv (4698 segments)\n",
      "[OK] 00001642-100507 -> fixed_9s_segment_of_each_label/00001642-100507_segments.csv (3685 segments)\n",
      "[OK] 00001643-100507 -> fixed_9s_segment_of_each_label/00001643-100507_segments.csv (6644 segments)\n",
      "[OK] 00001649-100507 -> fixed_9s_segment_of_each_label/00001649-100507_segments.csv (6931 segments)\n",
      "[OK] 00001656-100507 -> fixed_9s_segment_of_each_label/00001656-100507_segments.csv (7143 segments)\n",
      "[OK] 00001658-100507 -> fixed_9s_segment_of_each_label/00001658-100507_segments.csv (7312 segments)\n",
      "[OK] 00001661-100507 -> fixed_9s_segment_of_each_label/00001661-100507_segments.csv (3488 segments)\n",
      "[OK] 00001667-100507 -> fixed_9s_segment_of_each_label/00001667-100507_segments.csv (4660 segments)\n",
      "[OK] 00001669-100507 -> fixed_9s_segment_of_each_label/00001669-100507_segments.csv (5633 segments)\n",
      "[OK] 00001673-100507 -> fixed_9s_segment_of_each_label/00001673-100507_segments.csv (5882 segments)\n",
      "[OK] 00001677-100507 -> fixed_9s_segment_of_each_label/00001677-100507_segments.csv (3524 segments)\n",
      "[OK] 00001679-100507 -> fixed_9s_segment_of_each_label/00001679-100507_segments.csv (6580 segments)\n",
      "[OK] 00001681-100507 -> fixed_9s_segment_of_each_label/00001681-100507_segments.csv (5142 segments)\n",
      "[OK] 00001683-100507 -> fixed_9s_segment_of_each_label/00001683-100507_segments.csv (6942 segments)\n",
      "[OK] 00001687-100507 -> fixed_9s_segment_of_each_label/00001687-100507_segments.csv (7197 segments)\n",
      "[OK] 00001689-100507 -> fixed_9s_segment_of_each_label/00001689-100507_segments.csv (6887 segments)\n",
      "[OK] 00001695-100507 -> fixed_9s_segment_of_each_label/00001695-100507_segments.csv (6606 segments)\n",
      "[OK] 00001701-100507 -> fixed_9s_segment_of_each_label/00001701-100507_segments.csv (6838 segments)\n",
      "\n",
      "Summary:\n",
      "Processed: 146; success: 146; failed: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call batch runner\n",
    "batch_create_segments(\n",
    "    totals_csv=totals_csv,\n",
    "    output_dir=output_path,\n",
    "    events_dir=events_dir,\n",
    "    segment_length=9,\n",
    "    overlap=6,\n",
    "    total_durations_csv=totals_csv,  # the same file used by create_segment_csv_from_identifier for lookups\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv1",
   "language": "python",
   "name": "tfenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
