{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b439bc18-45d1-47ff-92c0-45786208baeb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d65384-4b63-40fa-a8de-3cec5a79d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import csv\n",
    "from scipy.signal import resample_poly\n",
    "from typing import Union, List, Dict, Tuple, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6641d3f-0402-4b79-bbe2-9c8f5d71cc28",
   "metadata": {},
   "source": [
    "## Feature Extractor function\n",
    "It works on a single 9s segment of 48kHz and returns a dictionary of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d82ae8-1f0d-4bcf-bfc4-09e2c4aea32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lpc_apnea_features(\n",
    "    audio: np.ndarray,\n",
    "    sr: int = 16000,\n",
    "    frame_length_ms: float = 25.0,\n",
    "    hop_length_ms: float = 10.0,\n",
    "    lpc_order: int = 12,\n",
    "    n_fft: int = 512,\n",
    "    delta_widths = (0, 3, 4, 5),\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute LPC-based spectral-envelope features (and derivative/std summaries)\n",
    "    intended for sleep-apnea-from-sound experiments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : np.ndarray\n",
    "        1D audio array (mono). Example: 9 seconds at 16 kHz.\n",
    "    sr : int\n",
    "        Sample rate (default 16000).\n",
    "    frame_length_ms : float\n",
    "        Frame length in milliseconds for short-time framing (default 25 ms).\n",
    "    hop_length_ms : float\n",
    "        Hop length in milliseconds (default 10 ms).\n",
    "    lpc_order : int\n",
    "        LPC order for each frame (default 12).\n",
    "    n_fft : int\n",
    "        Number of frequency bins to evaluate spectral envelope (default 512).\n",
    "    delta_widths : iterable\n",
    "        Iterable of integer delta widths to compute temporal deltas of the envelope.\n",
    "        We use 0 to indicate \"no delta\" (original envelope), others are passed\n",
    "        to librosa.feature.delta as `width=width*2+1` internally.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : dict\n",
    "        Dictionary with summary features. Keys include:\n",
    "          - 'env_mean' : mean over all frames and freqs of spectral envelope\n",
    "          - 'env_std'  : std over all frames and freqs of spectral envelope\n",
    "          - 'freq_deriv_mean' : mean of frequency-derivative (env diff along freq)\n",
    "          - 'freq_deriv_std'  : std of frequency-derivative\n",
    "          - 'delta_{w}_std'   : overall std of temporal delta of envelope for width w\n",
    "          - 'delta_{w}_mean'  : overall mean of temporal delta of envelope for width w\n",
    "        and also per-frequency / per-frame arrays are returned under keys if needed.\n",
    "    \"\"\"\n",
    "    # --- frame/hop sizes in samples\n",
    "    frame_len = int(round(sr * (frame_length_ms / 1000.0)))\n",
    "    hop_len = int(round(sr * (hop_length_ms / 1000.0)))\n",
    "    if frame_len <= lpc_order:\n",
    "        raise ValueError(\"frame length (samples) must be > lpc_order\")\n",
    "\n",
    "    # Short-time framing (librosa.util.frame helps but we'll use simple framing approach)\n",
    "    # Use librosa.util.frame to create a 2D array (frames x frame_len)\n",
    "    frames = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop_len).T  # shape (n_frames, frame_len)\n",
    "\n",
    "    n_frames = frames.shape[0]\n",
    "\n",
    "    # Frequency grid for spectral envelope\n",
    "    w, _ = scipy.signal.freqz([1.0], [1.0], worN=n_fft, fs=sr)  # returns freqs and response for placeholders\n",
    "    freqs = w[:n_fft] if len(w) >= n_fft else w\n",
    "\n",
    "    # Container for envelopes: shape (n_frames, n_fft)\n",
    "    envelopes = np.zeros((n_frames, n_fft), dtype=float)\n",
    "\n",
    "    for i in range(n_frames):\n",
    "        frame = frames[i] * np.hamming(frame_len)  # window\n",
    "        # compute LPC (librosa.lpc returns coefficients a: [1, -a1, -a2, ...] style)\n",
    "        try:\n",
    "            a = librosa.lpc(frame, order=lpc_order)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # if LPC fails due to ill-conditioned autocorrelation, fallback to zeros\n",
    "            a = np.zeros(lpc_order + 1, dtype=float)\n",
    "            a[0] = 1.0\n",
    "        # Compute frequency response of the all-pole filter 1/A(z) -> magnitude is spectral envelope\n",
    "        # scipy.signal.freqz expects numerator (b) and denominator (a). For LPC, numerator is 1.\n",
    "        freq_axis, h = scipy.signal.freqz([1.0], a, worN=n_fft, fs=sr)\n",
    "        mag = np.abs(h)\n",
    "        # Avoid zeros and stabilize with small epsilon\n",
    "        envelopes[i, :] = mag + 1e-12\n",
    "\n",
    "    # Global envelope statistics\n",
    "    env_mean = float(np.mean(envelopes))\n",
    "    env_std = float(np.std(envelopes))\n",
    "\n",
    "    # Frequency derivative (first diff along frequency axis)\n",
    "    freq_deriv = np.diff(envelopes, axis=1)  # shape (n_frames, n_fft-1)\n",
    "    freq_deriv_mean = float(np.mean(freq_deriv))\n",
    "    freq_deriv_std  = float(np.std(freq_deriv))\n",
    "\n",
    "    # Temporal deltas of the envelope:\n",
    "    # librosa.feature.delta uses width = 2 * N + 1 where N is the order.\n",
    "    # We'll map delta_width w -> librosa width = 2*w + 1 (so w=3 -> width 7)\n",
    "    delta_stats = {}\n",
    "    for w in delta_widths:\n",
    "        if w == 0:\n",
    "            # interpret width 0 as the original envelope (no delta)\n",
    "            delta_env = envelopes.copy()\n",
    "        else:\n",
    "            # compute delta along time axis for each frequency bin\n",
    "            # librosa expects shape (n_freq, n_frames) -> so transpose\n",
    "            delta_env = librosa.feature.delta(envelopes.T, width=(2*w + 1), order=1, axis=1).T\n",
    "            # librosa.feature.delta returns same shape as input\n",
    "        # overall summary\n",
    "        key_mean = f\"delta_{w}_mean\"\n",
    "        key_std  = f\"delta_{w}_std\"\n",
    "        delta_stats[key_mean] = float(np.mean(delta_env))\n",
    "        delta_stats[key_std]  = float(np.std(delta_env))\n",
    "\n",
    "    # Pack features\n",
    "    features = {\n",
    "        \"env_mean\": env_mean,\n",
    "        \"env_std\": env_std,\n",
    "        \"freq_deriv_mean\": freq_deriv_mean,\n",
    "        \"freq_deriv_std\": freq_deriv_std,\n",
    "    }\n",
    "    features.update(delta_stats)\n",
    "\n",
    "    # # Optionally include some arrays for deeper inspection (but keep them optional)\n",
    "    # features[\"_debug\"] = {\n",
    "    #     \"n_frames\": n_frames,\n",
    "    #     \"frame_len\": frame_len,\n",
    "    #     \"hop_len\": hop_len,\n",
    "    #     \"lpc_order\": lpc_order,\n",
    "    #     \"n_fft\": n_fft,\n",
    "    #     # Note: don't include entire envelopes if very large; included here for completeness\n",
    "    #     # Comment out if you want only summary stats:\n",
    "    #     # \"envelopes\": envelopes,\n",
    "    #     # \"freq_deriv\": freq_deriv,\n",
    "    # }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba695a98-47ec-4f6c-bf64-849cdaad0719",
   "metadata": {},
   "source": [
    "## The main iterator function\n",
    "For rapid parallel processing of all patients at the same time, Jess has created this framework that gets one patient ID at a time from the SBATCH. It is obtained by `os.environ.get()` and the patient ID is supplied by the `.sh` and `.sbatch` files combined. \n",
    "\n",
    "## Testing steps\n",
    "1. Before doing the final sbatch, we run this test notebook. Since we won't be getting parallel patient IDs from sbatch, we will manually set one patient ID and the input output directories (and comment out the original `os.environ.get()` block, as you can see in the below code).\n",
    "2. Call your custom feature function in the `#custom feature function goes here` part. Ensure the right indentation of the code.\n",
    "3. Your feature function is expected to return a dictionary of features (with feature names as keys). After we get the features in the `feats` variable, we add the labels to the dictionary. I also added the segment number to preserve sequence order information, in case we want to use it in RNN or LSTM.\n",
    "4. The rest of the code will write the features in a CSV file. The column names will be the feature names and \"label\" and \"segment_index\". The output file name will be the `patient_ID_feature_name.csv`.\n",
    "5. **MAKE SURE** to adjust the `output_file` variable in the `main()` function and adjust the feature_name part in the output csv file name.\n",
    "6. While testing your code in this notebook before the final SBATCH, **MAKE SURE** to adjust the `output_dir` variable in the `main()` function. We don't want to overwrite any other previously extracted features.\n",
    "7. The output file of this code will be a single csv file for a patient. The number of rows in this csv file will be equal to the number of segments for that patient in the big `.npy` file. Each row will contain the features extracted for a segment and the corresponding label.\n",
    "8. Once we see that the feature extraction function is working for a patient, we can move to sbatch.\n",
    "\n",
    "## SBATCH steps after you are sure that the code works ok\n",
    "1. We first need to prepare the python file. Currently, we have a ipynb notebook, which cannot be used with SBATCH. We need a `.py` file.\n",
    "    1. Make sure your python code does not plot anything. Even if it does, save the plot to a file in an output directory. We don't need plots for features, we can do it later. So, try to skip any plotting.\n",
    "    2. Copy all the codes in the cells of your notebook in a `.py` file.\n",
    "    3. When you were testing the notebook, you commented out the `os.environ.get()` code block with test variables. Make sure to uncomment this `os.environ.get()` code block and delete/comment out the test variables part where you assigned values for testing on a single patient. \n",
    "3. Go to `job_full_dataset.sh`\n",
    "    1. Adjust the `DATA_DIR`, `LABEL_DIR`, `OUTPUT_DIR`, `PATIENT_LIST`, `PYTHON_SCRIPT` variables.\n",
    "    2. Keep in mind that when you are using a absolute path, it starts with a slash `/`. But when you are copying paths of a folder by right clicking on the folder in the file explorer inside Jupyterlab, that starting slash `/` is not included. You will have to adjust it.\n",
    "4. After you are done with the `.sh` file, go to the `.sbatch` file.\n",
    "    1. Change the following lines\n",
    "        ```\n",
    "        \n",
    "        #SBATCH -c 32\n",
    "        #SBATCH --mem=64G\n",
    "        #SBATCH -t 0-01:00:00\n",
    "        #SBATCH --array=0-70   # Must exactly match number of patients\n",
    "\n",
    "        module load mamba/latest\n",
    "        source activate tfenv1\n",
    "\n",
    "        bash /scratch/sshuvo13/scratch_run_dir_sajib/LPC_sajib/job_full_dataset.sh\n",
    "        \n",
    "        ```\n",
    "        1. `-c 32` is the number of cores of your CPU. `--mem=64G` is how many GBs of RAMS. `-t 0-01:00:00` is in the format of `D-HH-MM-SS` format. For example, if you want the time cap for your code to be 1 day, 2 hours, 3 minutes, and 1 seconds, then you set `-t 1-02:03:04`. Your code might be finished before this time, and this is the hard deadline. If you code does not finish before this, SOL will terminate your session. So, it is better to set up a bit more time than you think you might need in the `-t` variable. The `--array` value should be set as the number of patients in the `DATA_DIR`. For example, if there are 73 male patients, `--array=0-72`.\n",
    "        2. Then you load mamba/latest and activate your environment as usual.\n",
    "        3. Then comes the most important part. You call the `.sh` file you created earlier using `bash`. Make sure that if you are using absolute path, your path starts with the slash `/`.\n",
    "5. Once you have your `.sbatch`, `.sh`, and `.py` files ready, recheck everything. Ensure that you set every paths correctly.\n",
    "6. **MAKE SURE THAT YOU SAVED ALL THREE FILES**.\n",
    "7. Then open a shell terminal within Jupyterlab by clicking on the plus (+) icon on top of the file browser, and scrolling all the way to the bottom.\n",
    "8. Run the sbatch file using:\n",
    "   ```\n",
    "   sbatch your_sbatch_file_name.sbatch\n",
    "   ```\n",
    "9. You can monitor your sbatch by running:\n",
    "    ```\n",
    "    squeue -u $USER\n",
    "    ```\n",
    "10. If you want to cancel a sbatch job, note the SBATCH job id (a big number) and run:\n",
    "    ```\n",
    "    scancel your_sbatch_job_ID\n",
    "    ```\n",
    "11. If you want to see something similar to the task manager of Windows, run `top` in the shell, and then press `shift + m` to sort the list. Press `q` to quit the `top` environment.\n",
    "12. If something is wrong in the `sbatch`, `sh`, or `py`, you will be able to see it in the `.err` files in the `./Logs/` directory, which will be in the same directory as your `.sbatch` file.\n",
    "13. If you have used `print()` function in your `.py` code, you will see the print outputs in the `.out` files in the `./Logs/` directory. Usually, the `.out` files get populated at the end of code execution, rather than being continuously populated as the code runs. \n",
    "\n",
    "14. After your `.sbatch` run is complete, you will find all the patient's feature `.csv` files in the output directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c60141-1c73-4be0-a84c-027682cf438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(patient_id, data_dir, label_dir, output_dir):\n",
    "\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    filename = f\"{patient_id}_segmented.npy\"\n",
    "    segments_file_path = os.path.join(data_dir, filename)\n",
    "    print(f\"Opening (memmap) {segments_file_path}\")\n",
    "    segments = np.load(segments_file_path, mmap_mode='r')  # <-- memory-mapped\n",
    "\n",
    "    if segments.ndim != 2:\n",
    "        print(f\"File {filename} does not contain a 2D array; skipping.\")\n",
    "        return\n",
    "\n",
    "    # load labels (still likely small)\n",
    "    label_file_name = f\"{patient_id}_segments_labels.npy\"\n",
    "    label_file_path = os.path.join(label_dir, label_file_name)\n",
    "    label_file = np.load(label_file_path)  # if this is huge, memmap it too\n",
    "\n",
    "    number_of_segments = segments.shape[0]\n",
    "    labels_subset = label_file[:number_of_segments]\n",
    "\n",
    "    # Prepare CSV output (write header)\n",
    "    output_file = os.path.join(output_dir, f\"{patient_id}_LPC.csv\")\n",
    "    header_written = False\n",
    "\n",
    "    # iterate by index to avoid copying the whole 'segments' at once\n",
    "    for i in range(number_of_segments):\n",
    "        try:\n",
    "            seg = segments[i]            # this is a memmap slice (view)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading segment {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # resample to 16k\n",
    "        try:\n",
    "            signal_16khz = resample_to_16k(seg, orig_sr=48000, target_sr=16000)\n",
    "        except Exception:\n",
    "            # fallback to librosa if needed\n",
    "            signal_16khz = librosa.resample(np.asarray(seg, dtype=np.float32), orig_sr=48000, target_sr=16000)\n",
    "\n",
    "\n",
    "        #custom feature function goes here\n",
    "        feats = compute_lpc_apnea_features(audio = signal_16khz)\n",
    "        # feats = {'Q_MFCC_avg': feats_raw}\n",
    "        feats['label'] = labels_subset[i] if i < len(labels_subset) else None\n",
    "        feats['segment_index'] = i\n",
    "\n",
    "        # write row immediately to CSV (append)\n",
    "        if not header_written:\n",
    "            with open(output_file, 'w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=list(feats.keys()))\n",
    "                writer.writeheader()\n",
    "                writer.writerow(feats)\n",
    "            header_written = True\n",
    "        else:\n",
    "            with open(output_file, 'a', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=list(feats.keys()))\n",
    "                writer.writerow(feats)\n",
    "\n",
    "    print(f\"Finished {filename}, wrote {output_file}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read environment variables set by job.sh\n",
    "    # patient_id = os.environ.get(\"PATIENT_ID\")\n",
    "    # data_dir = os.environ.get(\"DATA_DIR\")\n",
    "    # label_dir = os.environ.get(\"LABEL_DIR\")\n",
    "    # output_dir = os.environ.get(\"OUTPUT_DIR\")\n",
    "\n",
    "    patient_id = '00000995-100507'\n",
    "    data_dir = \"/scratch/sshuvo13/project_shared_folder_bspml_1/segmented_edfs/female_segmented_edfs\"\n",
    "    label_dir=\"/scratch/sshuvo13/project_shared_folder_bspml_1/rml_analysis/segment_csv_data/labels_of_each_segment\"\n",
    "    output_dir=\"/scratch/sshuvo13/project_shared_folder_bspml_1/whole_dataset_features/female/LPC_sajib\"\n",
    "\n",
    "    # Simple check\n",
    "    if not all([patient_id, data_dir, label_dir, output_dir]):\n",
    "        raise ValueError(\"Missing required environment variables: PATIENT_ID, DATA_DIR, LABEL_DIR, OUTPUT_DIR\")\n",
    "\n",
    "    # print(f\"\\nProcessing patient: {patient_id}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    main(patient_id, data_dir, label_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cec36-9ecd-4e03-acfa-88faab4f412b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv1",
   "language": "python",
   "name": "tfenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
